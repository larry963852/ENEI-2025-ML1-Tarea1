{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models and Regularization\n",
    "\n",
    "This notebook follows the assignment instructions to compare ordinary least squares, gradient descent, and regularized linear models on the California Housing and Bike Sharing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow below is intentionally concise: load data, standardize features, run linear baselines, explore regularization, and repeat the pipeline for the alternative dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_and_scale(X, y, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_train = np.asarray(y_train)\n",
    "    y_test = np.asarray(y_test)\n",
    "    return (\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        X_train_scaled,\n",
    "        X_test_scaled,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "\n",
    "def closed_form_ols(X, y):\n",
    "    X_design = np.c_[np.ones(X.shape[0]), X]\n",
    "    beta = np.linalg.pinv(X_design.T @ X_design) @ X_design.T @ y\n",
    "    intercept = beta[0]\n",
    "    coef = beta[1:]\n",
    "    return intercept, coef\n",
    "\n",
    "\n",
    "def predict_with_params(X, intercept, coef):\n",
    "    return X @ coef + intercept\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, learning_rate=0.01, n_iter=5000, tol=1e-8):\n",
    "    X_design = np.c_[np.ones(X.shape[0]), X]\n",
    "    beta = np.zeros(X_design.shape[1])\n",
    "    history = []\n",
    "    for i in range(n_iter):\n",
    "        preds = X_design @ beta\n",
    "        residuals = preds - y\n",
    "        grad = (2 / len(y)) * (X_design.T @ residuals)\n",
    "        beta -= learning_rate * grad\n",
    "        cost = np.mean(residuals ** 2)\n",
    "        history.append(cost)\n",
    "        if i > 0 and abs(history[-2] - cost) < tol:\n",
    "            break\n",
    "    intercept = beta[0]\n",
    "    coef = beta[1:]\n",
    "    return intercept, coef, history\n",
    "\n",
    "\n",
    "def summarize_performance(name, y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\"model\": name, \"mse\": mse, \"r2\": r2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A \u2013 California Housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "california = fetch_california_housing(as_frame=True)\n",
    "california_df = california.frame.copy()\n",
    "X_cal = california_df.drop(columns='MedHouseVal')\n",
    "y_cal = california_df['MedHouseVal']\n",
    "\n",
    "(\n",
    "    X_train_cal,\n",
    "    X_test_cal,\n",
    "    y_train_cal,\n",
    "    y_test_cal,\n",
    "    X_train_cal_scaled,\n",
    "    X_test_cal_scaled,\n",
    "    scaler_cal,\n",
    ") = split_and_scale(X_cal, y_cal)\n",
    "\n",
    "feature_names_cal = X_cal.columns.tolist()\n",
    "california_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Closed-form Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "ols_intercept_cal, ols_coef_cal = closed_form_ols(X_train_cal_scaled, y_train_cal)\n",
    "print(f\"Intercept: {ols_intercept_cal:.4f}\")\n",
    "\n",
    "ols_coefs_table_cal = pd.DataFrame({\n",
    "    'feature': feature_names_cal,\n",
    "    'coefficient': ols_coef_cal\n",
    "}).sort_values(by='coefficient', key=lambda s: s.abs(), ascending=False)\n",
    "ols_coefs_table_cal\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_cal_ols = predict_with_params(X_test_cal_scaled, ols_intercept_cal, ols_coef_cal)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test_cal, y_pred_cal_ols, alpha=0.4)\n",
    "lims = [min(y_test_cal.min(), y_pred_cal_ols.min()), max(y_test_cal.max(), y_pred_cal_ols.max())]\n",
    "plt.plot(lims, lims, color='black', linestyle='--', linewidth=1)\n",
    "plt.xlabel('True Median House Value')\n",
    "plt.ylabel('Predicted Median House Value')\n",
    "plt.title('California Housing: OLS Predictions vs. Truth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rates_cal = [0.01, 0.1]\n",
    "gd_results_cal = {}\n",
    "plt.figure(figsize=(7, 4))\n",
    "for lr in learning_rates_cal:\n",
    "    gd_intercept, gd_coef, history = gradient_descent(\n",
    "        X_train_cal_scaled, y_train_cal, learning_rate=lr, n_iter=5000\n",
    "    )\n",
    "    gd_results_cal[lr] = {\n",
    "        'intercept': gd_intercept,\n",
    "        'coef': gd_coef,\n",
    "        'history': history,\n",
    "    }\n",
    "    plt.plot(history, label=f\"lr={lr}\")\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Gradient Descent Cost Curves (California)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "comparison_rows = []\n",
    "for lr, result in gd_results_cal.items():\n",
    "    y_pred = predict_with_params(X_test_cal_scaled, result['intercept'], result['coef'])\n",
    "    mse = mean_squared_error(y_test_cal, y_pred)\n",
    "    comparison_rows.append({\n",
    "        'learning_rate': lr,\n",
    "        'iterations': len(result['history']),\n",
    "        'test_mse': mse,\n",
    "        'intercept_diff': abs(result['intercept'] - ols_intercept_cal),\n",
    "        'max_coef_diff': np.abs(result['coef'] - ols_coef_cal).max(),\n",
    "    })\n",
    "\n",
    "gd_comparison_cal = pd.DataFrame(comparison_rows)\n",
    "gd_comparison_cal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scikit-learn Baseline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "linreg_cal = LinearRegression()\n",
    "linreg_cal.fit(X_train_cal_scaled, y_train_cal)\n",
    "\n",
    "y_pred_cal_lr = linreg_cal.predict(X_test_cal_scaled)\n",
    "metrics_cal = [\n",
    "    summarize_performance('Closed-form OLS', y_test_cal, y_pred_cal_ols),\n",
    "    summarize_performance('Sklearn LinearRegression', y_test_cal, y_pred_cal_lr),\n",
    "]\n",
    "\n",
    "best_gd_lr = min(gd_results_cal, key=lambda lr: mean_squared_error(\n",
    "    y_test_cal, predict_with_params(X_test_cal_scaled, gd_results_cal[lr]['intercept'], gd_results_cal[lr]['coef'])\n",
    "))\n",
    "y_pred_best_gd = predict_with_params(\n",
    "    X_test_cal_scaled,\n",
    "    gd_results_cal[best_gd_lr]['intercept'],\n",
    "    gd_results_cal[best_gd_lr]['coef'],\n",
    ")\n",
    "metrics_cal.append(summarize_performance(f'Gradient Descent (lr={best_gd_lr})', y_test_cal, y_pred_best_gd))\n",
    "\n",
    "coef_diff = np.abs(linreg_cal.coef_ - ols_coef_cal).max()\n",
    "intercept_diff = abs(linreg_cal.intercept_ - ols_intercept_cal)\n",
    "print(f\"Max coefficient difference vs OLS: {coef_diff:.3e}\")\n",
    "print(f\"Intercept difference vs OLS: {intercept_diff:.3e}\")\n",
    "\n",
    "pd.DataFrame(metrics_cal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Ridge and Lasso Paths"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "alphas = np.logspace(-3, 2, 30)\n",
    "ridge_coefs_cal = []\n",
    "lasso_coefs_cal = []\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_cal_scaled, y_train_cal)\n",
    "    ridge_coefs_cal.append(ridge.coef_)\n",
    "\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X_train_cal_scaled, y_train_cal)\n",
    "    lasso_coefs_cal.append(lasso.coef_)\n",
    "\n",
    "ridge_coefs_cal = np.array(ridge_coefs_cal)\n",
    "lasso_coefs_cal = np.array(lasso_coefs_cal)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for idx, feature in enumerate(feature_names_cal):\n",
    "    plt.plot(alphas, np.abs(ridge_coefs_cal[:, idx]), label=feature)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('alpha (log scale)')\n",
    "plt.ylabel('|coefficient|')\n",
    "plt.title('Ridge Coefficient Paths (California)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for idx, feature in enumerate(feature_names_cal):\n",
    "    plt.plot(alphas, np.abs(lasso_coefs_cal[:, idx]), label=feature)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('alpha (log scale)')\n",
    "plt.ylabel('|coefficient|')\n",
    "plt.title('Lasso Coefficient Paths (California)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge gently shrinks all coefficients while Lasso rapidly pushes the weaker signals (especially `AveOccup`, `Population`, and `Longitude`) toward zero as regularization strengthens. `MedInc` remains dominant under both penalties because it carries most of the predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Cross-Validation for Regularization Strength"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "ridge_cv_cal = RidgeCV(alphas=alphas)\n",
    "ridge_cv_cal.fit(X_train_cal_scaled, y_train_cal)\n",
    "y_pred_cal_ridge = ridge_cv_cal.predict(X_test_cal_scaled)\n",
    "\n",
    "lasso_cv_cal = LassoCV(alphas=alphas, max_iter=20000, random_state=42)\n",
    "lasso_cv_cal.fit(X_train_cal_scaled, y_train_cal)\n",
    "y_pred_cal_lasso = lasso_cv_cal.predict(X_test_cal_scaled)\n",
    "\n",
    "cv_results_cal = pd.DataFrame([\n",
    "    {\n",
    "        'model': 'RidgeCV',\n",
    "        'best_alpha': ridge_cv_cal.alpha_,\n",
    "        'test_mse': mean_squared_error(y_test_cal, y_pred_cal_ridge),\n",
    "        'test_r2': r2_score(y_test_cal, y_pred_cal_ridge),\n",
    "    },\n",
    "    {\n",
    "        'model': 'LassoCV',\n",
    "        'best_alpha': lasso_cv_cal.alpha_,\n",
    "        'test_mse': mean_squared_error(y_test_cal, y_pred_cal_lasso),\n",
    "        'test_r2': r2_score(y_test_cal, y_pred_cal_lasso),\n",
    "    },\n",
    "])\n",
    "cv_results_cal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation prefers a modest Ridge penalty (alpha near the lower end of the grid) and a slightly stronger Lasso penalty, yielding minor improvements in test error while guarding against coefficient inflation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Polynomial Features and Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_cal_poly = poly.fit_transform(X_train_cal)\n",
    "X_test_cal_poly = poly.transform(X_test_cal)\n",
    "\n",
    "scaler_cal_poly = StandardScaler()\n",
    "X_train_cal_poly_scaled = scaler_cal_poly.fit_transform(X_train_cal_poly)\n",
    "X_test_cal_poly_scaled = scaler_cal_poly.transform(X_test_cal_poly)\n",
    "\n",
    "ridge_cv_cal_poly = RidgeCV(alphas=alphas)\n",
    "ridge_cv_cal_poly.fit(X_train_cal_poly_scaled, y_train_cal)\n",
    "y_pred_cal_ridge_poly = ridge_cv_cal_poly.predict(X_test_cal_poly_scaled)\n",
    "\n",
    "lasso_cv_cal_poly = LassoCV(alphas=alphas, max_iter=30000, random_state=42)\n",
    "lasso_cv_cal_poly.fit(X_train_cal_poly_scaled, y_train_cal)\n",
    "y_pred_cal_lasso_poly = lasso_cv_cal_poly.predict(X_test_cal_poly_scaled)\n",
    "\n",
    "poly_results_cal = pd.DataFrame([\n",
    "    {\n",
    "        'model': 'RidgeCV (poly degree 2)',\n",
    "        'best_alpha': ridge_cv_cal_poly.alpha_,\n",
    "        'test_mse': mean_squared_error(y_test_cal, y_pred_cal_ridge_poly),\n",
    "        'test_r2': r2_score(y_test_cal, y_pred_cal_ridge_poly),\n",
    "    },\n",
    "    {\n",
    "        'model': 'LassoCV (poly degree 2)',\n",
    "        'best_alpha': lasso_cv_cal_poly.alpha_,\n",
    "        'test_mse': mean_squared_error(y_test_cal, y_pred_cal_lasso_poly),\n",
    "        'test_r2': r2_score(y_test_cal, y_pred_cal_lasso_poly),\n",
    "    },\n",
    "])\n",
    "poly_results_cal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding quadratic terms explodes the feature count and multicollinearity, but Ridge keeps the expanded model stable with only a slight loss in accuracy. Lasso trims many polynomial interactions entirely, reinforcing how sparsity helps when the design matrix becomes redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D \u2013 Bike Sharing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Load Data and Inspect Seasonal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "bike_df = pd.read_csv('../data/hour.csv')\n",
    "bike_df['dteday'] = pd.to_datetime(bike_df['dteday'])\n",
    "\n",
    "season_names = {1: 'Spring', 2: 'Summer', 3: 'Fall', 4: 'Winter'}\n",
    "seasonal_mean = bike_df.groupby('season')['cnt'].mean().rename(index=season_names)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "seasonal_mean.plot(kind='bar', color=['#4c72b0', '#55a868', '#c44e52', '#8172b2'])\n",
    "plt.ylabel('Average Rentals (cnt)')\n",
    "plt.title('Average Rentals by Season')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "bike_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Prepare Features and Closed-form OLS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "bike_features = bike_df.drop(columns=['cnt', 'instant', 'dteday', 'casual', 'registered'])\n",
    "X_bike = bike_features\n",
    "y_bike = bike_df['cnt']\n",
    "\n",
    "(\n",
    "    X_train_bike,\n",
    "    X_test_bike,\n",
    "    y_train_bike,\n",
    "    y_test_bike,\n",
    "    X_train_bike_scaled,\n",
    "    X_test_bike_scaled,\n",
    "    scaler_bike,\n",
    ") = split_and_scale(X_bike, y_bike)\n",
    "\n",
    "feature_names_bike = X_bike.columns.tolist()\n",
    "\n",
    "ols_intercept_bike, ols_coef_bike = closed_form_ols(X_train_bike_scaled, y_train_bike)\n",
    "print(f\"Intercept: {ols_intercept_bike:.4f}\")\n",
    "\n",
    "ols_coefs_table_bike = pd.DataFrame({\n",
    "    'feature': feature_names_bike,\n",
    "    'coefficient': ols_coef_bike\n",
    "}).sort_values(by='coefficient', key=lambda s: s.abs(), ascending=False)\n",
    "ols_coefs_table_bike\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_bike_OLS = predict_with_params(X_test_bike_scaled, ols_intercept_bike, ols_coef_bike)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test_bike, y_pred_bike_OLS, alpha=0.4)\n",
    "lims = [min(y_test_bike.min(), y_pred_bike_OLS.min()), max(y_test_bike.max(), y_pred_bike_OLS.max())]\n",
    "plt.plot(lims, lims, color='black', linestyle='--', linewidth=1)\n",
    "plt.xlabel('True Rentals (cnt)')\n",
    "plt.ylabel('Predicted Rentals (cnt)')\n",
    "plt.title('Bike Sharing: OLS Predictions vs. Truth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rates_bike = [0.001, 0.01]\n",
    "gd_results_bike = {}\n",
    "plt.figure(figsize=(7, 4))\n",
    "for lr in learning_rates_bike:\n",
    "    gd_intercept, gd_coef, history = gradient_descent(\n",
    "        X_train_bike_scaled, y_train_bike, learning_rate=lr, n_iter=8000\n",
    "    )\n",
    "    gd_results_bike[lr] = {\n",
    "        'intercept': gd_intercept,\n",
    "        'coef': gd_coef,\n",
    "        'history': history,\n",
    "    }\n",
    "    plt.plot(history, label=f\"lr={lr}\")\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Gradient Descent Cost Curves (Bike)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "comparison_rows_bike = []\n",
    "for lr, result in gd_results_bike.items():\n",
    "    y_pred = predict_with_params(X_test_bike_scaled, result['intercept'], result['coef'])\n",
    "    mse = mean_squared_error(y_test_bike, y_pred)\n",
    "    comparison_rows_bike.append({\n",
    "        'learning_rate': lr,\n",
    "        'iterations': len(result['history']),\n",
    "        'test_mse': mse,\n",
    "        'intercept_diff': abs(result['intercept'] - ols_intercept_bike),\n",
    "        'max_coef_diff': np.abs(result['coef'] - ols_coef_bike).max(),\n",
    "    })\n",
    "\n",
    "gd_comparison_bike = pd.DataFrame(comparison_rows_bike)\n",
    "gd_comparison_bike\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Scikit-learn Baseline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "linreg_bike = LinearRegression()\n",
    "linreg_bike.fit(X_train_bike_scaled, y_train_bike)\n",
    "\n",
    "y_pred_bike_lr = linreg_bike.predict(X_test_bike_scaled)\n",
    "metrics_bike = [\n",
    "    summarize_performance('Closed-form OLS', y_test_bike, y_pred_bike_OLS),\n",
    "    summarize_performance('Sklearn LinearRegression', y_test_bike, y_pred_bike_lr),\n",
    "]\n",
    "\n",
    "best_gd_lr_bike = min(gd_results_bike, key=lambda lr: mean_squared_error(\n",
    "    y_test_bike,\n",
    "    predict_with_params(X_test_bike_scaled, gd_results_bike[lr]['intercept'], gd_results_bike[lr]['coef'])\n",
    "))\n",
    "y_pred_best_gd_bike = predict_with_params(\n",
    "    X_test_bike_scaled,\n",
    "    gd_results_bike[best_gd_lr_bike]['intercept'],\n",
    "    gd_results_bike[best_gd_lr_bike]['coef'],\n",
    ")\n",
    "metrics_bike.append(summarize_performance(f'Gradient Descent (lr={best_gd_lr_bike})', y_test_bike, y_pred_best_gd_bike))\n",
    "\n",
    "coef_diff_bike = np.abs(linreg_bike.coef_ - ols_coef_bike).max()\n",
    "intercept_diff_bike = abs(linreg_bike.intercept_ - ols_intercept_bike)\n",
    "print(f\"Max coefficient difference vs OLS: {coef_diff_bike:.3e}\")\n",
    "print(f\"Intercept difference vs OLS: {intercept_diff_bike:.3e}\")\n",
    "\n",
    "pd.DataFrame(metrics_bike)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Regularization Paths"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "ridge_coefs_bike = []\n",
    "lasso_coefs_bike = []\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train_bike_scaled, y_train_bike)\n",
    "    ridge_coefs_bike.append(ridge.coef_)\n",
    "\n",
    "    lasso = Lasso(alpha=alpha, max_iter=20000)\n",
    "    lasso.fit(X_train_bike_scaled, y_train_bike)\n",
    "    lasso_coefs_bike.append(lasso.coef_)\n",
    "\n",
    "ridge_coefs_bike = np.array(ridge_coefs_bike)\n",
    "lasso_coefs_bike = np.array(lasso_coefs_bike)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for idx, feature in enumerate(feature_names_bike):\n",
    "    plt.plot(alphas, np.abs(ridge_coefs_bike[:, idx]), label=feature)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('alpha (log scale)')\n",
    "plt.ylabel('|coefficient|')\n",
    "plt.title('Ridge Coefficient Paths (Bike)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for idx, feature in enumerate(feature_names_bike):\n",
    "    plt.plot(alphas, np.abs(lasso_coefs_bike[:, idx]), label=feature)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('alpha (log scale)')\n",
    "plt.ylabel('|coefficient|')\n",
    "plt.title('Lasso Coefficient Paths (Bike)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As regularization strengthens, Lasso again zeroes out weaker weather and binary indicators first, while Ridge keeps every variable but dampens their influence. Season-related terms stay relatively large, aligning with the observed seasonal demand swings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Cross-Validation and Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "ridge_cv_bike = RidgeCV(alphas=alphas)\n",
    "ridge_cv_bike.fit(X_train_bike_scaled, y_train_bike)\n",
    "y_pred_bike_ridge = ridge_cv_bike.predict(X_test_bike_scaled)\n",
    "\n",
    "lasso_cv_bike = LassoCV(alphas=alphas, max_iter=30000, random_state=42)\n",
    "lasso_cv_bike.fit(X_train_bike_scaled, y_train_bike)\n",
    "y_pred_bike_lasso = lasso_cv_bike.predict(X_test_bike_scaled)\n",
    "\n",
    "cv_results_bike = pd.DataFrame([\n",
    "    {\n",
    "        'model': 'RidgeCV',\n",
    "        'best_alpha': ridge_cv_bike.alpha_,\n",
    "        'test_mse': mean_squared_error(y_test_bike, y_pred_bike_ridge),\n",
    "        'test_r2': r2_score(y_test_bike, y_pred_bike_ridge),\n",
    "    },\n",
    "    {\n",
    "        'model': 'LassoCV',\n",
    "        'best_alpha': lasso_cv_bike.alpha_,\n",
    "        'test_mse': mean_squared_error(y_test_bike, y_pred_bike_lasso),\n",
    "        'test_r2': r2_score(y_test_bike, y_pred_bike_lasso),\n",
    "    },\n",
    "])\n",
    "cv_results_bike\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "poly_bike = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_bike_poly = poly_bike.fit_transform(X_train_bike)\n",
    "X_test_bike_poly = poly_bike.transform(X_test_bike)\n",
    "\n",
    "scaler_bike_poly = StandardScaler()\n",
    "X_train_bike_poly_scaled = scaler_bike_poly.fit_transform(X_train_bike_poly)\n",
    "X_test_bike_poly_scaled = scaler_bike_poly.transform(X_test_bike_poly)\n",
    "\n",
    "ridge_cv_bike_poly = RidgeCV(alphas=alphas)\n",
    "ridge_cv_bike_poly.fit(X_train_bike_poly_scaled, y_train_bike)\n",
    "y_pred_bike_ridge_poly = ridge_cv_bike_poly.predict(X_test_bike_poly_scaled)\n",
    "\n",
    "lasso_cv_bike_poly = LassoCV(alphas=alphas, max_iter=40000, random_state=42)\n",
    "lasso_cv_bike_poly.fit(X_train_bike_poly_scaled, y_train_bike)\n",
    "y_pred_bike_lasso_poly = lasso_cv_bike_poly.predict(X_test_bike_poly_scaled)\n",
    "\n",
    "poly_results_bike = pd.DataFrame([\n",
    "    {\n",
    "        'model': 'RidgeCV (poly degree 2)',\n",
    "        'best_alpha': ridge_cv_bike_poly.alpha_,\n",
    "        'test_mse': mean_squared_error(y_test_bike, y_pred_bike_ridge_poly),\n",
    "        'test_r2': r2_score(y_test_bike, y_pred_bike_ridge_poly),\n",
    "    },\n",
    "    {\n",
    "        'model': 'LassoCV (poly degree 2)',\n",
    "        'best_alpha': lasso_cv_bike_poly.alpha_,\n",
    "        'test_mse': mean_squared_error(y_test_bike, y_pred_bike_lasso_poly),\n",
    "        'test_r2': r2_score(y_test_bike, y_pred_bike_lasso_poly),\n",
    "    },\n",
    "])\n",
    "poly_results_bike\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The enlarged bike feature space benefits slightly from Ridge, which tolerates correlated seasonal and weather terms. Lasso prunes redundant indicators created by the polynomial expansion, preventing the model from memorizing noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}